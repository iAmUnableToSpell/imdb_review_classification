{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and important constants\n",
    "import tensorflow as tf\n",
    "from keras import layers, metrics\n",
    "from keras import losses\n",
    "import numpy as np\n",
    "import util\n",
    "import time\n",
    "from random import shuffle\n",
    "\n",
    "# other classifiers for comparison\n",
    "from sklearn import svm\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "import sklearn.metrics\n",
    "from sklearn.naive_bayes import (\n",
    "    BernoulliNB,\n",
    "    ComplementNB,\n",
    "    MultinomialNB,\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "max_features = 1000\n",
    "embedding_dim = 16\n",
    "sequence_length = 250\n",
    "epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import aclImbd\n",
    "# This downloads a lot of text files, half of which are never referenced. \n",
    "# For more information, please see the function itself\n",
    "util.prepare_data_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vectorization layer - embedding specfic\n",
    "vectorize_layer = layers.TextVectorization(standardize=util.custom_standardization, max_tokens=max_features, output_mode='int', output_sequence_length=sequence_length)\n",
    "\n",
    "def vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return vectorize_layer(text), label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup NLTK - scikit-learn specfic\n",
    "# NLTK dataset for comparison, using same number of features. Very slow when compared to keras' text vectorization for the same number of tokens!\n",
    "nltk.download(['vader_lexicon', 'movie_reviews','punkt', 'stopwords', 'names', 'averaged_perceptron_tagger'])\n",
    "\n",
    "top_positive, top_negative = util.prepare_nltk(max_features)\n",
    "nltk_features = [\n",
    "    (util.nltk_extract_features(nltk.corpus.movie_reviews.raw(review), top_positive), \"pos\")\n",
    "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"pos\"])\n",
    "]\n",
    "nltk_features.extend([\n",
    "    (util.nltk_extract_features(nltk.corpus.movie_reviews.raw(review), top_positive), \"neg\")\n",
    "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"neg\"])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets and adapt vectorization layer - embedding specfic\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory('aclImdb/train', batch_size=batch_size, validation_split=0.2, subset='training', seed=seed)\n",
    "raw_val_ds = tf.keras.utils.text_dataset_from_directory('aclImdb/train', batch_size=batch_size, validation_split=0.2, subset='validation', seed=seed)\n",
    "raw_test_ds = tf.keras.utils.text_dataset_from_directory('aclImdb/test', batch_size=batch_size)\n",
    "train_text = raw_train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache datasets for fast retrieval to speed up training - embedding specfic\n",
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding model\n",
    "model_embedding = tf.keras.Sequential([\n",
    "  layers.Embedding(max_features, embedding_dim),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.GlobalAveragePooling1D(),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Dense(1)])\n",
    "\n",
    "model_embedding.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer='adam',\n",
    "              metrics=[tf.metrics.BinaryAccuracy(threshold=0.0), metrics.AUC(from_logits=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NTLK models\n",
    "classifiers = {\n",
    "    \"BernoulliNB\": BernoulliNB(),\n",
    "    \"ComplementNB\": ComplementNB(),\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "    \"KNeighborsClassifier\": KNeighborsClassifier(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(),\n",
    "    \"MLPClassifier\": MLPClassifier(max_iter=1000),\n",
    "    \"AdaBoostClassifier\": AdaBoostClassifier(),\n",
    "}\n",
    "\n",
    "performance = {}\n",
    "\n",
    "train_count = len(nltk_features) // 2\n",
    "shuffle(nltk_features)\n",
    "for name, sklearn_classifier in classifiers.items():\n",
    "    classifier = nltk.classify.SklearnClassifier(sklearn_classifier)\n",
    "    st = time.time()\n",
    "    classifier.train(nltk_features[:train_count])\n",
    "    ed = time.time()\n",
    "    accuracy = nltk.classify.accuracy(classifier, nltk_features[train_count:])\n",
    "\n",
    "    y_true, y_score = [], []\n",
    "\n",
    "    for i, (feats, label_true) in enumerate(nltk_features[train_count:]):\n",
    "        label_predicted = classifier.classify(feats)\n",
    "        y_true.append(1 if label_true == 'pos' else 0)\n",
    "        y_score.append(1 if label_predicted == 'pos' else 0)\n",
    "\n",
    "    # Precision-Recall AUC\n",
    "    precision, recall, _ = sklearn.metrics.precision_recall_curve(y_true, y_score, pos_label=1)\n",
    "    pr_auc = sklearn.metrics.auc(recall, precision)\n",
    "    fpr, tpr, _ = sklearn.metrics.roc_curve(y_true, y_score, pos_label=1)\n",
    "    roc_auc = sklearn.metrics.auc(fpr, tpr)\n",
    "\n",
    "    roc_auc = sklearn.metrics.auc(fpr, tpr)\n",
    "    performance[name] = {}\n",
    "    performance[name]['acc'] = accuracy\n",
    "    performance[name]['time'] = ed-st\n",
    "    performance[name]['auc'] = roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Embedding Model\n",
    "start = time.time()\n",
    "history = model_embedding.fit(train_ds, validation_data=val_ds, epochs=epochs)\n",
    "end = time.time()\n",
    "util.evaluate_model_keras(model_embedding, test_ds, history)\n",
    "print(f\"Training Time Embedding: {end-start}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate NLTK models\n",
    "for name in performance:\n",
    "    print(f'Model: {name}')\n",
    "    acc = performance[name]['acc']\n",
    "    time = performance[name]['time']\n",
    "    auc = performance[name]['auc']\n",
    "    print(f'   Accuracy: {acc}')\n",
    "    print(f'   auROC: {auc}')\n",
    "    print(f'   Train Time: {time}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7dece438061d77ad4af5befad1fc0f0bd0321126e9bcead865e9783a682176da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
